/* model_inference.cpp
 * Placeholder showing how to call a tiny model (TFLite Micro) on device.
 * Real code requires including TFLite Micro, model data, interpreter setup etc.
 * See TensorFlow Lite Micro docs for exact setup.
 */

// Pseudo-code / comments only

/*
#include "tensorflow/lite/micro/all_ops_resolver.h"
#include "tensorflow/lite/micro/micro_interpreter.h"
#include "model_data.h"  // compiled tflite model as C array

int run_inference(float *input_data, int input_len) {
    // Setup resolver, tensor arena, interpreter...
    // Feed input_data, invoke, read output
    // Return action / classification
}
*/
